# RAI-Eval
Resposible AI is very important and we need to see how how resposible the LLMs we have today are. Models like ChatGPT, Calude, GROK, Perplexity, LLaMa, and so much more. In this repo we propose a way to evaluate robustness and fairness with a prompts dataset to see benchmarking across all the models and get an Idea of how responsible they are.
